{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample uwb data to 1hz (convert to function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import field_neuro as fn\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load your metadata sheet (if not already loaded)\n",
    "meta = pd.read_excel(r'Y:\\Data\\FieldProject\\FieldMission5\\metadata.xlsx')  # Adjust the path to your metadata file\n",
    "\n",
    "# Manually define each animals tag shortid, tag start time, and tag end time (or suspected death time) to solve the tag switching across animals issue. \n",
    "# information derived from my lab notebook notes and metadata. \n",
    "# note that this should never happen again, and should be avoided at all costs. Ideally, one UWB tag goes with one animal for the entire experiment. \n",
    "\n",
    "data = [\n",
    "    ('MFEL', '3012', '2024-06-22 19:00:00', '2024-06-27 12:00:00'), ## estimated. Note first day of data lost\n",
    "    ('MFEL', '304c', '2024-06-22 12:00:00', '2024-06-30 22:36:00'), ## estimated\n",
    "    ('MFEL', '304d', '2024-06-30 22:36:00', '2024-07-03 17:36:00'), \n",
    "    ('MTAM', '3015', '2024-06-22 19:00:00', '2024-06-27 22:50:00'),\n",
    "    ('MTAM', '3040', '2024-06-27 22:50:00', '2024-07-01 22:53:00'),\n",
    "    ('MTAM', '303f', '2024-07-01 22:53:00', '2024-07-04 09:00:00'), ## caught by zipple, no headcap/tag\n",
    "    ('MDAN', '3020', '2024-06-22 19:00:00', '2024-06-27 20:20:00'),\n",
    "    ('MDAN', '303d', '2024-06-27 20:20:00', '2024-07-02 22:53:00'),\n",
    "    ('MDAN', '3043', '2024-07-02 22:53:00', '2024-07-03 20:52:00'), ## handtrapped in tunnel under Z4\n",
    "    ('MARV', '3019', '2024-06-22 19:00:00', '2024-06-25 17:40:00'),\n",
    "    ('MARV', '3028', '2024-06-25 17:40:00', '2024-06-27 20:05:00'),\n",
    "    ('MARV', '303f', '2024-06-27 20:05:00', '2024-06-30 22:20:00'), ## found dead under Z3\n",
    "    ('MMTS', '3016', '2024-06-22 19:00:00', '2024-06-27 20:45:00'),\n",
    "    ('MMTS', '3046', '2024-06-27 20:45:00', '2024-07-01 01:24:00'),\n",
    "    ('MMTS', '303c', '2024-07-01 01:24:00', '2024-07-03 17:34:00'), ## trapped under z5\n",
    "    ('MMTO', '3011', '2024-06-22 19:00:00', '2024-06-25 16:10:00'),\n",
    "    ('MMTO', '3038', '2024-06-25 16:10:00', '2024-06-27 21:00:00'),\n",
    "    ('MMTO', '3041', '2024-06-27 21:00:00', '2024-07-01 22:20:00'),\n",
    "    ('MMTO', '3042', '2024-07-01 22:20:00', '2024-07-03 17:34:00'), ## trapped under z6, no baseplate, headcap, or microtag\n",
    "    ('MRUF', '301e', '2024-06-22 19:00:00', '2024-07-27 22:10:00'),\n",
    "    ('MRUF', '304e', '2024-06-27 22:10:00', '2024-07-03 20:30:00'), ## handtrapped in grass between anchor 12 and 15\n",
    "    ('MMTN', '301c', '2024-06-22 19:00:00', '2024-06-27 22:36:00'),\n",
    "    ('MMTN', '3044', '2024-06-27 22:36:00', '2024-07-01 02:07:00'),\n",
    "    ('MMTN', '3043', '2024-07-01 02:07:00', '2024-07-01 21:00:00'), ## sacced during battery swap\n",
    "    ('FROS', '3010', '2024-06-22 22:00:00', '2024-06-27 20:40:00'),\n",
    "    ('FROS', '304a', '2024-06-27 20:40:00', '2024-07-01 01:38:00'),\n",
    "    ('FROS', '3049', '2024-07-01 01:38:00', '2024-07-03 17:17:00'), ## trapped in z5\n",
    "    ('FAUM', '3014', '2024-06-22 22:00:00', '2024-06-25 17:40:00'),\n",
    "    ('FAUM', '3033', '2024-06-25 17:40:00', '2024-06-27 21:30:00'),\n",
    "    ('FAUM', '304b', '2024-06-27 21:30:00', '2024-07-03 20:43:00'), ## trapped under z3\n",
    "    ('FSBR', '300d', '2024-06-22 22:00:00', '2024-06-27 22:22:00'),\n",
    "    ('FSBR', '304f', '2024-06-27 22:22:00', '2024-07-03 18:43:00'), ## trapped in z11\n",
    "    ('FMRT', '3013', '2024-06-22 22:00:00', '2024-06-25 18:30:00'),\n",
    "    ('FMRT', '302d', '2024-06-25 18:30:00', '2024-06-27 20:30:00'),\n",
    "    ('FMRT', '3047', '2024-06-27 20:30:00', '2024-07-03 21:00:00'), ## trapped near anchor 22 along wall\n",
    "    ('FMAR', '3018', '2024-06-22 22:00:00', '2024-06-27 16:10:00'),\n",
    "    ('FMAR', '3037', '2024-06-27 16:10:00', '2024-06-27 20:53:00'),\n",
    "    ('FMAR', '3045', '2024-06-27 20:53:00', '2024-07-03 17:50:00'), ## trapped in z6\n",
    "    ('FBRY', '301d', '2024-06-22 22:00:00', '2024-06-25 16:10:00'),\n",
    "    ('FBRY', '303a', '2024-06-25 16:10:00', '2024-06-27 21:05:00'),\n",
    "    ('FBRY', '3048', '2024-06-27 21:05:00', '2024-07-03 17:43:00'), ## trapped in z6\n",
    "    ('FTSS', '3021', '2024-06-22 22:00:00', '2024-06-30 22:20:00'), ## found dead under z3\n",
    "    ('FCRN', '301f', '2024-06-22 22:00:00', '2024-06-27 21:55:00'),\n",
    "    ('FCRN', '303e', '2024-06-27 21:55:00', '2024-07-03 18:22:00'), ## trapped in z8\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "meta_code_hex = pd.DataFrame(data, columns=['code', 'hex_id', 'tag_start_time', 'tag_end_time'])\n",
    "\n",
    "# Confirm that tag_start_time and tag_end_time are timezone-naive\n",
    "meta_code_hex['tag_start_time'] = pd.to_datetime(meta_code_hex['tag_start_time'])\n",
    "meta_code_hex['tag_end_time'] = pd.to_datetime(meta_code_hex['tag_end_time'])\n",
    "\n",
    "# Check for any rows where tag_start_time is greater than tag_end_time\n",
    "invalid_intervals = meta_code_hex[meta_code_hex['tag_start_time'] > meta_code_hex['tag_end_time']]\n",
    "\n",
    "# Display any invalid intervals\n",
    "# print(\"Invalid intervals (start time > end time):\")\n",
    "# print(invalid_intervals)\n",
    "\n",
    "# Display the DataFrame\n",
    "# meta_code_hex\n",
    "\n",
    "\n",
    "### view the data frame, optional\n",
    "# Connect to the SQLite database, doesnt require loading entire dataset into RAM\n",
    "# conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# # List all tables in the database\n",
    "# tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "# print(\"Tables in the database:\", tables)\n",
    "\n",
    "# # Describe the structure of a specific table\n",
    "# table_name = 'bravo_sql_databse_2024_CCVBehaviorTrial'\n",
    "# table_info = pd.read_sql_query(f\"PRAGMA table_info({table_name});\", conn)\n",
    "# print(f\"Structure of the table {table_name}:\", table_info)\n",
    "\n",
    "# # View the first few rows of the table\n",
    "# sample_data = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 5;\", conn)\n",
    "\n",
    "# print(sample_data)\n",
    "\n",
    "# # Get the total number of rows in the table; takes a very long time to run\n",
    "# # row_count = pd.read_sql_query(f\"SELECT COUNT(*) FROM {table_name};\", conn)\n",
    "# # print(f\"Total number of rows in the table {table_name}: {row_count.iloc[0, 0]}\")\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All chunk testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 0 starting at offset 0 with 10000000 rows\n",
      "Rows after interval filtering: 8620913 (removed 1379087)\n",
      "Rows after velocity filtering: 3479500 (removed 5141413)\n",
      "Number of single jumps detected and removed: 141\n",
      "Rows after jump filtering: 3479359 (removed 141)\n",
      "Rows after dropping duplicates: 384246 (removed 3095113)\n",
      "Final number of rows in this chunk: 384246\n",
      "\n",
      "Processing chunk 1 starting at offset 10000000 with 10000000 rows\n",
      "Rows after interval filtering: 6705576 (removed 3294424)\n",
      "Rows after velocity filtering: 2832715 (removed 3872861)\n",
      "Number of single jumps detected and removed: 23\n",
      "Rows after jump filtering: 2832692 (removed 23)\n",
      "Rows after dropping duplicates: 286282 (removed 2546410)\n",
      "Final number of rows in this chunk: 286282\n",
      "\n",
      "Processing chunk 2 starting at offset 20000000 with 10000000 rows\n",
      "Rows after interval filtering: 10000000 (removed 0)\n",
      "Rows after velocity filtering: 4163680 (removed 5836320)\n",
      "Number of single jumps detected and removed: 16\n",
      "Rows after jump filtering: 4163664 (removed 16)\n",
      "Rows after dropping duplicates: 454536 (removed 3709128)\n",
      "Final number of rows in this chunk: 454536\n",
      "\n",
      "Processing chunk 3 starting at offset 30000000 with 10000000 rows\n",
      "Rows after interval filtering: 10000000 (removed 0)\n",
      "Rows after velocity filtering: 4231058 (removed 5768942)\n",
      "Number of single jumps detected and removed: 9\n",
      "Rows after jump filtering: 4231049 (removed 9)\n",
      "Rows after dropping duplicates: 417806 (removed 3813243)\n",
      "Final number of rows in this chunk: 417806\n",
      "\n",
      "Processing chunk 4 starting at offset 40000000 with 10000000 rows\n",
      "Rows after interval filtering: 10000000 (removed 0)\n",
      "Rows after velocity filtering: 3981508 (removed 6018492)\n",
      "Number of single jumps detected and removed: 5\n",
      "Rows after jump filtering: 3981503 (removed 5)\n",
      "Rows after dropping duplicates: 544843 (removed 3436660)\n",
      "Final number of rows in this chunk: 544843\n",
      "\n",
      "Processing chunk 5 starting at offset 50000000 with 10000000 rows\n",
      "Rows after interval filtering: 9999977 (removed 23)\n",
      "Rows after velocity filtering: 4098493 (removed 5901484)\n",
      "Number of single jumps detected and removed: 47\n",
      "Rows after jump filtering: 4098446 (removed 47)\n",
      "Rows after dropping duplicates: 514840 (removed 3583606)\n",
      "Final number of rows in this chunk: 514840\n",
      "\n",
      "Processing chunk 6 starting at offset 60000000 with 10000000 rows\n",
      "Rows after interval filtering: 9996821 (removed 3179)\n",
      "Rows after velocity filtering: 4045383 (removed 5951438)\n",
      "Number of single jumps detected and removed: 16\n",
      "Rows after jump filtering: 4045367 (removed 16)\n",
      "Rows after dropping duplicates: 438877 (removed 3606490)\n",
      "Final number of rows in this chunk: 438877\n",
      "\n",
      "Processing chunk 7 starting at offset 70000000 with 10000000 rows\n",
      "Rows after interval filtering: 10000000 (removed 0)\n",
      "Rows after velocity filtering: 3828695 (removed 6171305)\n",
      "Number of single jumps detected and removed: 16\n",
      "Rows after jump filtering: 3828679 (removed 16)\n",
      "Rows after dropping duplicates: 458531 (removed 3370148)\n",
      "Final number of rows in this chunk: 458531\n",
      "\n",
      "Processing chunk 8 starting at offset 80000000 with 4774293 rows\n",
      "Rows after interval filtering: 4773597 (removed 696)\n",
      "Rows after velocity filtering: 1648790 (removed 3124807)\n",
      "Number of single jumps detected and removed: 30\n",
      "Rows after jump filtering: 1648760 (removed 30)\n",
      "Rows after dropping duplicates: 291942 (removed 1356818)\n",
      "Final number of rows in this chunk: 291942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
      "C:\\Users\\ayalab\\AppData\\Local\\Temp\\ipykernel_1884\\1964030606.py:160: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks have been concatenated and saved to Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv. Chunk files have been deleted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 10000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "chunk_index = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Output directory\n",
    "output_dir = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb'\n",
    "\n",
    "# Process data in chunks\n",
    "while has_more_data:\n",
    "    # Fetch a chunk of data from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT shortid,\n",
    "           timestamp,\n",
    "           location_x, \n",
    "           location_y, \n",
    "           zones,\n",
    "           alias,\n",
    "           alternateid,\n",
    "           groupnames\n",
    "    FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    uwb = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if uwb.empty:\n",
    "        has_more_data = False\n",
    "    else:\n",
    "        print(f\"\\nProcessing chunk {chunk_index} starting at offset {offset} with {len(uwb)} rows\")\n",
    "\n",
    "        # Initial row count\n",
    "        initial_rows = len(uwb)\n",
    "\n",
    "        # Create field time with ms\n",
    "        uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "        # Adjust for timezone (e.g., UTC-4)\n",
    "        uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "        # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "        uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "        # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "        origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "        # Calculate the noon_day column\n",
    "        uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # Convert location coordinates to meters from inches (from the wiser software)\n",
    "        uwb['location_x'] *= 0.0254\n",
    "        uwb['location_y'] *= 0.0254\n",
    "\n",
    "        # Convert shortid to hex_id\n",
    "        uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "        # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "        uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "        # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "        uwb = uwb[\n",
    "            (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "            ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "        ]\n",
    "\n",
    "        # Count rows after interval filtering\n",
    "        after_interval_filter = len(uwb)\n",
    "        print(f\"Rows after interval filtering: {after_interval_filter} (removed {initial_rows - after_interval_filter})\")\n",
    "\n",
    "        # Merge in metadata info and select final columns and order them as needed\n",
    "        uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "        \n",
    "        # FILTERING and smoothing on the chunk. \n",
    "        \n",
    "        # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "        uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "        # Calculate the time difference in seconds\n",
    "        uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "        # Calculate distances and velocities between consecutive points within each animal\n",
    "        uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                                  (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "        uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "        # Additional filtering based on velocity, acceleration, and large jumps\n",
    "        # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "        velocity_threshold = 2  # meters/second\n",
    "        uwb_before_velocity_filter = len(uwb)\n",
    "        uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "        after_velocity_filter = len(uwb)\n",
    "        print(f\"Rows after velocity filtering: {after_velocity_filter} (removed {uwb_before_velocity_filter - after_velocity_filter})\")\n",
    "\n",
    "        # Detect sudden jumps\n",
    "        jump_threshold = 2  # in meters\n",
    "        uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "        single_jumps = uwb['is_jump'].sum()\n",
    "        print(f\"Number of single jumps detected and removed: {single_jumps}\")\n",
    "\n",
    "        # Filter out these jumps\n",
    "        uwb_before_jump_filter = len(uwb)\n",
    "        uwb = uwb[~uwb['is_jump']]\n",
    "        after_jump_filter = len(uwb)\n",
    "        print(f\"Rows after jump filtering: {after_jump_filter} (removed {uwb_before_jump_filter - after_jump_filter})\")\n",
    "\n",
    "        # Group consecutive points that fall within a set time window (in seconds)\n",
    "        uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "        uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 30).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "        uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "        uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "        # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "        # Create a new column `field_time_s` that truncates the `field_time` to seconds\n",
    "        uwb['field_time_s'] = uwb['field_time'].dt.floor('s')\n",
    "\n",
    "        # Ensure only one entry per code and field_time_s\n",
    "        uwb_before_drop_duplicates = len(uwb)\n",
    "        uwb = uwb.drop_duplicates(subset=['code', 'field_time_s']).reset_index(drop=True)\n",
    "        after_drop_duplicates = len(uwb)\n",
    "        print(f\"Rows after dropping duplicates: {after_drop_duplicates} (removed {uwb_before_drop_duplicates - after_drop_duplicates})\")\n",
    "\n",
    "        final_rows = len(uwb)\n",
    "        print(f\"Final number of rows in this chunk: {final_rows}\")\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'noon_day', 'field_time', 'smoothed_x', 'smoothed_y', 'location_x', 'location_y', 'zones', 'hex_id']]\n",
    "\n",
    "        # Write the processed chunk to a separate CSV file\n",
    "        chunk_output_file = os.path.join(output_dir, f'T005_uwb_1hz_chunk_{chunk_index}.csv')\n",
    "        uwb.to_csv(chunk_output_file, index=False)\n",
    "        \n",
    "        # Increment the chunk index\n",
    "        chunk_index += 1\n",
    "    \n",
    "    # Update offset to fetch the next chunk in the next iteration\n",
    "    offset += chunk_size\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# List all chunk files\n",
    "chunk_files = glob.glob(os.path.join(output_dir, 'T005_uwb_1hz_chunk_*.csv'))\n",
    "\n",
    "# Read and concatenate all chunk files\n",
    "all_data = pd.concat([pd.read_csv(f) for f in chunk_files])\n",
    "\n",
    "# Write the concatenated data to the final output file\n",
    "final_output_file = os.path.join(output_dir, 'T005_uwb_1hz.csv')\n",
    "all_data.to_csv(final_output_file, index=False)\n",
    "\n",
    "# Optionally, remove the chunk files to free up space\n",
    "for f in chunk_files:\n",
    "    os.remove(f)\n",
    "\n",
    "print(f\"All chunks have been concatenated and saved to {final_output_file}. Chunk files have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 10000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Output file\n",
    "output_file = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv'\n",
    "\n",
    "# Process data in chunks\n",
    "while has_more_data:\n",
    "    # Fetch a chunk of data from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT shortid,\n",
    "           timestamp,\n",
    "           location_x, \n",
    "           location_y, \n",
    "           zones,\n",
    "           alias,\n",
    "           alternateid,\n",
    "           groupnames\n",
    "    FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    uwb = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if uwb.empty:\n",
    "        has_more_data = False\n",
    "    else:\n",
    "        print(f\"\\nProcessing chunk starting at offset {offset} with {len(uwb)} rows\")\n",
    "\n",
    "        # Initial row count\n",
    "        initial_rows = len(uwb)\n",
    "\n",
    "        # Create field time with ms\n",
    "        uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "        # Adjust for timezone (e.g., UTC-4)\n",
    "        uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "        # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "        uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "        # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "        origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "        # Calculate the noon_day column\n",
    "        uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # Convert location coordinates to meters from inches (from the wiser software)\n",
    "        uwb['location_x'] *= 0.0254\n",
    "        uwb['location_y'] *= 0.0254\n",
    "\n",
    "        # Convert shortid to hex_id\n",
    "        uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "        # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "        uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "        # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "        uwb = uwb[\n",
    "            (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "            ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "        ]\n",
    "\n",
    "        # Count rows after interval filtering\n",
    "        after_interval_filter = len(uwb)\n",
    "        print(f\"Rows after interval filtering: {after_interval_filter} (removed {initial_rows - after_interval_filter})\")\n",
    "\n",
    "        # Merge in metadata info and select final columns and order them as needed\n",
    "        uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "        \n",
    "        # FILTERING and smoothing on the chunk. \n",
    "        \n",
    "        # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "        uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "        # Calculate the time difference in seconds\n",
    "        uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "        # Calculate distances and velocities between consecutive points within each animal\n",
    "        uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                                  (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "        uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "        # Show descriptive statistics for velocity and distance before filtering\n",
    "        # print(\"\\nVelocity and Distance - Before Filtering\")\n",
    "        # print(uwb[['velocity', 'distance']].describe())\n",
    "\n",
    "        # Additional filtering based on velocity, acceleration, and large jumps\n",
    "        # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "        velocity_threshold = 3  # meters/second\n",
    "        uwb_before_velocity_filter = len(uwb)\n",
    "        uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "        after_velocity_filter = len(uwb)\n",
    "        print(f\"Rows after velocity filtering: {after_velocity_filter} (removed {uwb_before_velocity_filter - after_velocity_filter})\")\n",
    "\n",
    "        # Show descriptive statistics for velocity and distance after velocity filtering\n",
    "        # print(\"\\nVelocity and Distance - After Velocity Filtering\")\n",
    "        # print(uwb[['velocity', 'distance']].describe())\n",
    "\n",
    "        # Detect sudden jumps\n",
    "        jump_threshold = 2.5  # in meters\n",
    "        uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "        single_jumps = uwb['is_jump'].sum()\n",
    "        print(f\"Number of single jumps detected and removed: {single_jumps}\")\n",
    "\n",
    "        # Filter out these jumps\n",
    "        uwb_before_jump_filter = len(uwb)\n",
    "        uwb = uwb[~uwb['is_jump']]\n",
    "        after_jump_filter = len(uwb)\n",
    "        print(f\"Rows after jump filtering: {after_jump_filter} (removed {uwb_before_jump_filter - after_jump_filter})\")\n",
    "\n",
    "        # Show descriptive statistics for distance after jump filtering\n",
    "        # print(\"\\nDistance - After Jump Filtering\")\n",
    "        # print(uwb['distance'].describe())\n",
    "\n",
    "        # round the time_diff to time_diff_s (seconds) for grouping procedure\n",
    "        uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "        \n",
    "        # Group consecutive points that fall within a set time window (in seconds)\n",
    "        uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 10).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "        uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "        uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "        # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "        # Create a new column `field_time_s` that truncates the `field_time` to seconds\n",
    "        uwb['field_time_s'] = uwb['field_time'].dt.floor('s')\n",
    "        \n",
    "        # print(uwb.head())\n",
    "\n",
    "        # Ensure only one entry per code and field_time_s\n",
    "        uwb_before_drop_duplicates = len(uwb)\n",
    "        uwb = uwb.drop_duplicates(subset=['code', 'field_time_s']).reset_index(drop=True)\n",
    "        after_drop_duplicates = len(uwb)\n",
    "        print(f\"Rows after dropping duplicates: {after_drop_duplicates} (removed {uwb_before_drop_duplicates - after_drop_duplicates})\")\n",
    "\n",
    "        final_rows = len(uwb)\n",
    "        print(f\"Final number of rows in this chunk: {final_rows}\")\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'noon_day', 'field_time', 'smoothed_x', 'smoothed_y', 'location_x', 'location_y', 'zones', 'hex_id']]\n",
    "\n",
    "        # Append the processed chunk to the CSV file\n",
    "        uwb.to_csv(output_file, mode='a', header=not offset, index=False)\n",
    "    \n",
    "    # Update offset to fetch the next chunk in the next iteration\n",
    "    offset += chunk_size\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 10000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Output file\n",
    "output_file = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv'\n",
    "\n",
    "# Process data in chunks\n",
    "while has_more_data:\n",
    "    # Fetch a chunk of data from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT shortid,\n",
    "           timestamp,\n",
    "           location_x, \n",
    "           location_y, \n",
    "           zones,\n",
    "           alias,\n",
    "           alternateid,\n",
    "           groupnames\n",
    "    FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "    ORDER BY shortid, timestamp\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    uwb = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if uwb.empty:\n",
    "        has_more_data = False\n",
    "    else:\n",
    "        print(f\"\\nProcessing chunk starting at offset {offset} with {len(uwb)} rows\")\n",
    "\n",
    "        # Initial row count\n",
    "        initial_rows = len(uwb)\n",
    "\n",
    "        # Create field time with ms\n",
    "        uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "        # Adjust for timezone (e.g., UTC-4)\n",
    "        uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "        # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "        uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "        # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "        origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "        # Calculate the noon_day column\n",
    "        uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # Convert location coordinates to meters from inches (from the wiser software)\n",
    "        uwb['location_x'] *= 0.0254\n",
    "        uwb['location_y'] *= 0.0254\n",
    "\n",
    "        # Convert shortid to hex_id\n",
    "        uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "        # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "        uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "        # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "        uwb = uwb[\n",
    "            (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "            ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "        ]\n",
    "\n",
    "        # Count rows after interval filtering\n",
    "        after_interval_filter = len(uwb)\n",
    "        print(f\"Rows after interval filtering: {after_interval_filter} (removed {initial_rows - after_interval_filter})\")\n",
    "\n",
    "        # Merge in metadata info and select final columns and order them as needed\n",
    "        uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "\n",
    "        # FILTERING and smoothing on the chunk. \n",
    "        \n",
    "        # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "        uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "        # Calculate the time difference in seconds\n",
    "        uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "        # Calculate distances and velocities between consecutive points within each animal\n",
    "        uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                                  (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "        uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "        # Show descriptive statistics for velocity and distance before filtering\n",
    "        print(\"\\nVelocity and Distance - Before Filtering\")\n",
    "        print(uwb[['velocity', 'distance']].describe())\n",
    "\n",
    "        # Additional filtering based on velocity, acceleration, and large jumps\n",
    "        # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "        velocity_threshold = 3  # meters/second\n",
    "        uwb_before_velocity_filter = len(uwb)\n",
    "        uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "        after_velocity_filter = len(uwb)\n",
    "        print(f\"Rows after velocity filtering: {after_velocity_filter} (removed {uwb_before_velocity_filter - after_velocity_filter})\")\n",
    "\n",
    "        # Show descriptive statistics for velocity and distance after velocity filtering\n",
    "        print(\"\\nVelocity and Distance - After Velocity Filtering\")\n",
    "        print(uwb[['velocity', 'distance']].describe())\n",
    "\n",
    "        # Detect sudden jumps\n",
    "        jump_threshold = 2.5  # in meters\n",
    "        uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "        single_jumps = uwb['is_jump'].sum()\n",
    "        print(f\"Number of single jumps detected and removed: {single_jumps}\")\n",
    "\n",
    "        # Filter out these jumps\n",
    "        uwb_before_jump_filter = len(uwb)\n",
    "        uwb = uwb[~uwb['is_jump']]\n",
    "        after_jump_filter = len(uwb)\n",
    "        print(f\"Rows after jump filtering: {after_jump_filter} (removed {uwb_before_jump_filter - after_jump_filter})\")\n",
    "\n",
    "        # Show descriptive statistics for distance after jump filtering\n",
    "        print(\"\\nDistance - After Jump Filtering\")\n",
    "        print(uwb['distance'].describe())\n",
    "\n",
    "        # round the time_diff to time_diff_s (seconds) for grouping procedure\n",
    "        uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "        \n",
    "        # Group consecutive points that fall within a set time window (in seconds)\n",
    "        uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 10).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "        uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "        uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "        # Ensure uniqueness by dropping duplicates on the field_time for each code\n",
    "        uwb_before_drop_duplicates = len(uwb)\n",
    "        uwb = uwb.drop_duplicates(subset=['code', 'field_time'])\n",
    "        after_drop_duplicates = len(uwb)\n",
    "        print(f\"Rows after dropping duplicates: {after_drop_duplicates} (removed {uwb_before_drop_duplicates - after_drop_duplicates})\")\n",
    "\n",
    "        # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "        uwb = uwb.groupby(['code', uwb['field_time'].dt.floor('s')]).first().reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "        final_rows = len(uwb)\n",
    "        print(f\"Final number of rows in this chunk: {final_rows}\")\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'noon_day', 'field_time', 'smoothed_x', 'smoothed_y', 'location_x', 'location_y', 'zones', 'hex_id', 'alias', 'groupnames']]\n",
    "\n",
    "        # Append the processed chunk to the CSV file\n",
    "        uwb.to_csv(output_file, mode='a', header=not offset, index=False)\n",
    "    \n",
    "    # Update offset to fetch the next chunk in the next iteration\n",
    "    offset += chunk_size\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fp = r\"Y:\\Data\\FieldProject\\Output\"\n",
    "df = pd.read_csv(r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv')  # Adjust the path to your metadata file\n",
    "len(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 10000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Output file\n",
    "output_file = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv'\n",
    "\n",
    "# Process data in chunks\n",
    "while has_more_data:\n",
    "    # Fetch a chunk of data from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT shortid,\n",
    "           timestamp,\n",
    "           location_x, \n",
    "           location_y, \n",
    "           zones,\n",
    "           alias,\n",
    "           alternateid,\n",
    "           groupnames\n",
    "    FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "    ORDER BY shortid, timestamp\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    uwb = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if uwb.empty:\n",
    "        has_more_data = False\n",
    "    else:\n",
    "        print(f\"\\nProcessing chunk starting at offset {offset} with {len(uwb)} rows\")\n",
    "\n",
    "        # Initial row count\n",
    "        initial_rows = len(uwb)\n",
    "\n",
    "        # Create field time with ms\n",
    "        uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "        # Adjust for timezone (e.g., UTC-4)\n",
    "        uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "        # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "        uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "        # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "        origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "        # Calculate the noon_day column\n",
    "        uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # Convert location coordinates to meters from inches (from the wiser software)\n",
    "        uwb['location_x'] *= 0.0254\n",
    "        uwb['location_y'] *= 0.0254\n",
    "\n",
    "        # Convert shortid to hex_id\n",
    "        uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "        # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "        uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "        # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "        uwb = uwb[\n",
    "            (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "            ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "        ]\n",
    "\n",
    "        # Count rows after interval filtering\n",
    "        after_interval_filter = len(uwb)\n",
    "        print(f\"Rows after interval filtering: {after_interval_filter} (removed {initial_rows - after_interval_filter})\")\n",
    "\n",
    "        # Merge in metadata info and select final columns and order them as needed\n",
    "        uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "\n",
    "        # FILTERING and smoothing on the chunk. \n",
    "        \n",
    "        # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "        uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "        # Calculate the time difference in seconds\n",
    "        uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "        # Calculate distances and velocities between consecutive points within each animal\n",
    "        uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                                  (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "        uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "        # Additional filtering based on velocity, acceleration, and large jumps\n",
    "        # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "        velocity_threshold = 3  # meters/second\n",
    "        uwb_before_velocity_filter = len(uwb)\n",
    "        uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "        after_velocity_filter = len(uwb)\n",
    "        print(f\"Rows after velocity filtering: {after_velocity_filter} (removed {uwb_before_velocity_filter - after_velocity_filter})\")\n",
    "\n",
    "        # Detect sudden jumps\n",
    "        jump_threshold = 2.5  # in meters\n",
    "        uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "        single_jumps = uwb['is_jump'].sum()\n",
    "        print(f\"Number of single jumps detected and removed: {single_jumps}\")\n",
    "\n",
    "        # Filter out these jumps\n",
    "        uwb_before_jump_filter = len(uwb)\n",
    "        uwb = uwb[~uwb['is_jump']]\n",
    "        after_jump_filter = len(uwb)\n",
    "        print(f\"Rows after jump filtering: {after_jump_filter} (removed {uwb_before_jump_filter - after_jump_filter})\")\n",
    "\n",
    "        # round the time_diff to time_diff_s (seconds) for grouping procedure\n",
    "        uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "        \n",
    "        # Group consecutive points that fall within a set time window (in seconds)\n",
    "        uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 10).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "        uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "        uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "        # Ensure uniqueness by dropping duplicates on the field_time for each code\n",
    "        uwb_before_drop_duplicates = len(uwb)\n",
    "        uwb = uwb.drop_duplicates(subset=['code', 'field_time'])\n",
    "        after_drop_duplicates = len(uwb)\n",
    "        print(f\"Rows after dropping duplicates: {after_drop_duplicates} (removed {uwb_before_drop_duplicates - after_drop_duplicates})\")\n",
    "\n",
    "        # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "        uwb = uwb.groupby(['code', uwb['field_time'].dt.floor('s')]).first().reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "        final_rows = len(uwb)\n",
    "        print(f\"Final number of rows in this chunk: {final_rows}\")\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'noon_day', 'field_time', 'smoothed_x', 'smoothed_y', 'location_x', 'location_y', 'zones', 'hex_id', 'alias', 'groupnames']]\n",
    "\n",
    "        # Append the processed chunk to the CSV file\n",
    "        uwb.to_csv(output_file, mode='a', header=not offset, index=False)\n",
    "    \n",
    "    # Update offset to fetch the next chunk in the next iteration\n",
    "    offset += chunk_size\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 10000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Output file\n",
    "output_file = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv'\n",
    "\n",
    "# Process data in chunks\n",
    "while has_more_data:\n",
    "    # Fetch a chunk of data from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT shortid,\n",
    "           timestamp,\n",
    "           location_x, \n",
    "           location_y, \n",
    "           zones,\n",
    "           alias,\n",
    "           alternateid,\n",
    "           groupnames\n",
    "    FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "    ORDER BY shortid, timestamp\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    uwb = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if uwb.empty:\n",
    "        has_more_data = False\n",
    "    else:\n",
    "        # Perform your data processing on `uwb` here\n",
    "\n",
    "        # Create field time with ms\n",
    "        uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "        # Adjust for timezone (e.g., UTC-4)\n",
    "        uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "        # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "        uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "        # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "        origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "        # Calculate the noon_day column\n",
    "        uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # Convert location coordinates to meters from inches (from the wiser software)\n",
    "        uwb['location_x'] *= 0.0254\n",
    "        uwb['location_y'] *= 0.0254\n",
    "\n",
    "        # Convert shortid to hex_id\n",
    "        uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "        # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "        uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "        # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "        uwb = uwb[\n",
    "            (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "            ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "        ]\n",
    "\n",
    "        # Merge in metadata info and select final columns and order them as needed\n",
    "        uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "\n",
    "        uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "\n",
    "        # FILTERING and smoothing on the chunk. \n",
    "        \n",
    "        # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "        uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "        # Calculate the time difference in seconds\n",
    "        uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "        # Calculate distances and velocities between consecutive points within each animal\n",
    "        uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                                  (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "        uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "        # Additional filtering based on velocity, acceleration, and large jumps\n",
    "        # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "        velocity_threshold = 3  # meters/second\n",
    "        uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "\n",
    "        # Detect sudden jumps\n",
    "        jump_threshold = 2.5  # in meters\n",
    "        uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "        single_jumps = uwb['is_jump'].sum()\n",
    "        print(f\"Number of single jumps removed: {single_jumps}\")\n",
    "\n",
    "        # Filter out these jumps\n",
    "        uwb = uwb[~uwb['is_jump']]\n",
    "\n",
    "        # round the time_diff to time_diff_s (seconds) for grouping procedure\n",
    "        uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "        \n",
    "        # Group consecutive points that fall within a set time window (in seconds)\n",
    "        uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 10).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "        uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "        uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "        # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "        uwb = uwb.groupby(['code', uwb['field_time'].dt.floor('s')]).first().reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "        uwb = uwb[['trial','sex','code','noon_day','field_time','smoothed_x','smoothed_y','location_x','location_y','zones','hex_id','alias','groupnames']]\n",
    "\n",
    "        # Append the processed chunk to the CSV file\n",
    "        uwb.to_csv(output_file, mode='a', header=not offset, index=False)\n",
    "    \n",
    "    # Update offset to fetch the next chunk in the next iteration\n",
    "    offset += chunk_size\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single chunk testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## single chunk testing script. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database, doesn't require loading the entire dataset into RAM. Database is approximately 84 million rows worth of data\n",
    "conn = sqlite3.connect(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb.db\")\n",
    "\n",
    "# Define a chunk size (number of rows to fetch at a time)\n",
    "chunk_size = 50000000  # Adjust based on your memory capacity\n",
    "\n",
    "# Initialize variables for iteration\n",
    "offset = 0\n",
    "has_more_data = True\n",
    "\n",
    "# Perform a single iteration for testing\n",
    "query = f\"\"\"\n",
    "SELECT shortid,\n",
    "       timestamp,\n",
    "       location_x, \n",
    "       location_y, \n",
    "       zones,\n",
    "       alias,\n",
    "       alternateid,\n",
    "       groupnames\n",
    "FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "ORDER BY shortid, timestamp\n",
    "LIMIT {chunk_size} OFFSET {offset}\n",
    "\"\"\"\n",
    "\n",
    "# Read a chunk of data into a DataFrame\n",
    "uwb = pd.read_sql_query(query, conn)\n",
    "\n",
    "if uwb.empty:\n",
    "    has_more_data = False\n",
    "else:\n",
    "    # Perform your data processing on `uwb` here\n",
    "\n",
    "    # Create field time with ms\n",
    "    uwb['field_time'] = pd.to_datetime(uwb['timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "    # Adjust for timezone (e.g., UTC-4)\n",
    "    uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "    # Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "    uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "    # Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "    origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "    # Calculate the noon_day column\n",
    "    uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "    # Convert location coordinates to meters from inches (from the wiser software)\n",
    "    uwb['location_x'] *= 0.0254\n",
    "    uwb['location_y'] *= 0.0254\n",
    "\n",
    "    # Convert shortid to hex_id\n",
    "    uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "    # Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "    uwb = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "    # Filter rows based on the time interval; remove the rows where the uwb field_time doesn't fall in the meta_code_hex interval\n",
    "    uwb = uwb[\n",
    "        (uwb['field_time'] >= uwb['tag_start_time']) &\n",
    "        ((uwb['tag_end_time'].isna()) | (uwb['field_time'] < uwb['tag_end_time']))\n",
    "    ]\n",
    "\n",
    "    # Merge in metadata info and select final columns and order them as needed\n",
    "    uwb = pd.merge(uwb, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "\n",
    "    uwb = uwb[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones', 'location_x', 'location_y', 'alias', 'groupnames']]\n",
    "\n",
    "    # FILTERING and smoothing on the chunk. \n",
    "    \n",
    "    # Sort the data by animal and time to ensure proper calculation of velocity\n",
    "    uwb = uwb.sort_values(by=['code', 'field_time'])\n",
    "\n",
    "    # Calculate the time difference in seconds\n",
    "    uwb['time_diff'] = uwb.groupby('code')['field_time'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "\n",
    "    # Calculate distances and velocities between consecutive points within each animal\n",
    "    uwb['distance'] = np.sqrt((uwb['location_x'] - uwb.groupby('code')['location_x'].shift())**2 + \n",
    "                              (uwb['location_y'] - uwb.groupby('code')['location_y'].shift())**2)\n",
    "    uwb['velocity'] = uwb['distance'] / uwb['time_diff']\n",
    "\n",
    "    # Additional filtering based on velocity, acceleration, and large jumps\n",
    "    # Apply the velocity filter: remove any rows where the velocity exceeds a set threshold (e.g., 2 meters/second)\n",
    "    velocity_threshold = 3  # meters/second\n",
    "    uwb = uwb[(uwb['velocity'] <= velocity_threshold) | (uwb['velocity'].isna())]\n",
    "\n",
    "    # Detect sudden jumps\n",
    "    jump_threshold = 2.5  # in meters\n",
    "    uwb['is_jump'] = (uwb['distance'] > jump_threshold)\n",
    "\n",
    "    single_jumps = uwb['is_jump'].sum()\n",
    "    print(f\"Number of single jumps removed: {single_jumps}\")\n",
    "\n",
    "    # Filter out these jumps\n",
    "    uwb = uwb[~uwb['is_jump']]\n",
    "\n",
    "    # round the time_diff to time_diff_s (seconds) for grouping procedure\n",
    "    uwb['time_diff_s'] = np.ceil(uwb['time_diff']).astype(int)\n",
    "    \n",
    "    # Group consecutive points that fall within a set time window (in seconds)\n",
    "    uwb['tw_group'] = uwb.groupby('code')['time_diff_s'].apply(lambda x: (x > 10).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Smoothing: Apply a 30-data point (i.e., rows) rolling average within each group; window sets points; min_periods of 1 means whatever data will be used if not enough\n",
    "    uwb['smoothed_x'] = uwb.groupby(['code', 'tw_group'])['location_x'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "    uwb['smoothed_y'] = uwb.groupby(['code', 'tw_group'])['location_y'].transform(lambda y: y.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "    # Downsample to 1Hz by taking the first point for each second for each animal\n",
    "    uwb = uwb.groupby(['code', uwb['field_time'].dt.floor('S')]).first().reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "    uwb = uwb[['trial','sex','code','noon_day','field_time','smoothed_x','smoothed_y','location_x','location_y','zones','hex_id','alias','groupnames']]\n",
    "\n",
    "    # Display the first few rows to verify\n",
    "    print(uwb.head())\n",
    "\n",
    "    # (Optionally) Write the first chunk to a CSV to verify\n",
    "    uwb.to_csv(r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD APPROACH\n",
    "\n",
    "# Define your SQL query\n",
    "query = \"\"\"\n",
    "SELECT shortid, \n",
    "       calculation_error,\n",
    "       location_x, \n",
    "       location_y, \n",
    "       anchors_used,\n",
    "       battery_voltage,\n",
    "       zones,\n",
    "       alias,\n",
    "       alternateid,\n",
    "       groupnames,\n",
    "       CAST(timestamp / 1000 AS INTEGER) as timestamp_sec, # this removes the ms values\n",
    "       MIN(timestamp) as original_timestamp\n",
    "FROM bravo_sql_databse_2024_CCVBehaviorTrial\n",
    "GROUP BY shortid, timestamp_sec\n",
    "ORDER BY shortid, timestamp_sec\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and write the results directly to a CSV file\n",
    "output_file = r'Y:\\Data\\FieldProject\\FieldMission5\\uwb\\T005_uwb_1hz.csv'\n",
    "\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow([i[0] for i in cursor.description])\n",
    "    \n",
    "    # Write the rows\n",
    "    writer.writerows(cursor.fetchall())\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Data has been exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the 1hz data file and merge in the most important metadata information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "uwb = pd.read_csv(r\"Y:\\Data\\FieldProject\\FieldMission5\\uwb\\uwb_1hz.csv\")\n",
    "\n",
    "# Create field time with ms\n",
    "uwb['field_time'] = pd.to_datetime(uwb['original_timestamp'], unit='ms', origin='unix', utc=True)\n",
    "\n",
    "# Adjust for timezone (e.g., UTC-4)\n",
    "uwb['field_time'] = uwb['field_time'] - pd.Timedelta(hours=4)\n",
    "\n",
    "# Remove timezone information after adjusting for -4 hours; keep field_time timezone naive\n",
    "uwb['field_time'] = uwb['field_time'].dt.tz_localize(None)\n",
    "\n",
    "# Define the origin point (start of the trial). First day of trial data lost due to me leaving the sql database open\n",
    "origin = pd.Timestamp('2024-06-22 12:00:00')\n",
    "\n",
    "# Calculate the noon_day column\n",
    "uwb['noon_day'] = np.ceil((uwb['field_time'] - origin) / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# Convert location coordinates to meters from inches (from the wiser software)\n",
    "uwb['location_x'] *= 0.0254\n",
    "uwb['location_y'] *= 0.0254\n",
    "\n",
    "# Convert shortid to hex_id\n",
    "uwb['hex_id'] = uwb['shortid'].apply(lambda x: format(x, 'x'))\n",
    "\n",
    "# Merge on hex_id first, allowing multiple rows where hex_id matches; brings in both versions of the match with meta_code hex\n",
    "merged = uwb.merge(meta_code_hex, on='hex_id', how='left') \n",
    "\n",
    "# Filter rows based on the time interval; remove the rows where the uwb field_time doesnt fall in the meta_code_hex interval\n",
    "filtered = merged[\n",
    "    (merged['field_time'] >= merged['tag_start_time']) &\n",
    "    ((merged['tag_end_time'].isna()) | (merged['field_time'] < merged['tag_end_time']))\n",
    "]\n",
    "\n",
    "## Note: Need to come back and confirm that the grouping by code here is correct\n",
    "uwb_filtered = filtered.copy()\n",
    "\n",
    "# Merge in metadata info and select final columns and order them as needed\n",
    "\n",
    "# Merge uwb_filtered with the metadata sheet on the 'code' column\n",
    "uwb_merged = pd.merge(uwb_filtered, meta[['code', 'sex', 'trial']], on='code', how='left')\n",
    "\n",
    "# Display the merged DataFrame to verify\n",
    "# uwb_merged.head()\n",
    "\n",
    "uwb_filtered = uwb_merged[['trial', 'sex', 'code', 'hex_id', 'noon_day', 'field_time', 'zones',\n",
    "                            'location_x', 'location_y', 'alias', 'groupnames', 'original_timestamp', \n",
    "                            'anchors_used', 'calculation_error', 'battery_voltage']]\n",
    "\n",
    "\n",
    "# Display the first few rows to verify\n",
    "# uwb_filtered.head()\n",
    "\n",
    "uwb_filtered.to_csv(r'Y:\\Data\\FieldProject\\Results\\T005_uwb_1hz_clean.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "field_neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
